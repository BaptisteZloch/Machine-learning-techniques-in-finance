{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 15:07:49.146767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-18 15:07:49.681900: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-18 15:07:49.681922: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-18 15:07:52.302685: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-18 15:07:52.303035: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-18 15:07:52.303054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "\n",
    "from quant_invest_lab.data_provider import download_crypto_historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "tickers = [\"BTC-USDT\", \"ETH-USDT\", \"XRP-USDT\", \"LTC-USDT\", \"BCH-USDT\", \"EOS-USDT\"]\n",
    "timeframe = \"1day\"\n",
    "\n",
    "for ticker in tickers:    \n",
    "    df = download_crypto_historical_data(ticker, timeframe).loc[\"2020-11-20\":]\n",
    "    df[\"Log_Close\"] = np.log(df.Close.apply(lambda x: 1.0 if x == 0.0 else x))\n",
    "    df[\"Returns\"] = df.Close.pct_change()\n",
    "    df[\"Log_Returns\"] = df.Log_Close.pct_change()\n",
    "    df[\"Log_Volume\"] = np.log(np.abs(df.Volume.apply(lambda x: 1.0 if x == 0.0 else x)))\n",
    "    df[\"Vol20\"] = df.Log_Returns.rolling(20).std()\n",
    "    df[\"EMA20\"] = df.Close.ewm(20).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    data[ticker] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN values\n",
    "for ticker in data:\n",
    "    data[ticker] = data[ticker].fillna(method=\"ffill\")\n",
    "# Split the data into training and test sets\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for ticker in data:\n",
    "    ticker_data = data[ticker]\n",
    "    train_data[ticker], test_data[ticker] = train_test_split(\n",
    "        ticker_data, test_size=0.2, shuffle=False\n",
    "    )\n",
    "# Normalize the training data\n",
    "scaler = StandardScaler()\n",
    "for ticker in train_data:\n",
    "    train_data[ticker] = pd.DataFrame(\n",
    "        scaler.fit_transform(train_data[ticker]),\n",
    "        columns=train_data[ticker].columns,\n",
    "        index=train_data[ticker].index,\n",
    "    )\n",
    "# Normalize the test data using the same scaler object\n",
    "for ticker in test_data:\n",
    "    test_data[ticker] = pd.DataFrame(\n",
    "        scaler.transform(test_data[ticker]),\n",
    "        columns=test_data[ticker].columns,\n",
    "        index=test_data[ticker].index,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in train_data:\n",
    "    train_data[ticker][\"Target\"] = (\n",
    "        train_data[ticker][\"Close\"].shift(-5) > train_data[ticker][\"Close\"]\n",
    "    ).astype(int)\n",
    "for ticker in test_data:\n",
    "    test_data[ticker][\"Target\"] = (\n",
    "        test_data[ticker][\"Close\"].shift(-5) > test_data[ticker][\"Close\"]\n",
    "    ).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select N tickers for example purposes\n",
    "n_ExampleSymbols = 2\n",
    "\n",
    "train_data = dict(list(train_data.items())[:n_ExampleSymbols])\n",
    "test_data = dict(list(test_data.items())[:n_ExampleSymbols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 15:09:48.356760: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-18 15:09:48.383908: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-18 15:09:48.383978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-154PLAM): /proc/driver/nvidia/version does not exist\n",
      "2023-05-18 15:09:48.402500: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 7s 97ms/step - loss: 0.6809 - accuracy: 0.5782 - val_loss: 0.7138 - val_accuracy: 0.4755\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.6696 - accuracy: 0.5993 - val_loss: 0.7739 - val_accuracy: 0.4685\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.6660 - accuracy: 0.6081 - val_loss: 0.8256 - val_accuracy: 0.4685\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.6641 - accuracy: 0.6028 - val_loss: 0.8263 - val_accuracy: 0.4685\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.6634 - accuracy: 0.6046 - val_loss: 0.8243 - val_accuracy: 0.4685\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.6637 - accuracy: 0.6011 - val_loss: 0.8224 - val_accuracy: 0.4685\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.6644 - accuracy: 0.6081 - val_loss: 0.8114 - val_accuracy: 0.4685\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.6645 - accuracy: 0.5993 - val_loss: 0.8605 - val_accuracy: 0.4685\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6636 - accuracy: 0.6011Restoring model weights from the end of the best epoch: 1.\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.6636 - accuracy: 0.6011 - val_loss: 0.8158 - val_accuracy: 0.4685\n",
      "Epoch 9: early stopping\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 7s 98ms/step - loss: 0.6934 - accuracy: 0.5272 - val_loss: 0.6923 - val_accuracy: 0.5105\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.6909 - accuracy: 0.5589 - val_loss: 0.6918 - val_accuracy: 0.5175\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.6870 - accuracy: 0.5782 - val_loss: 0.6937 - val_accuracy: 0.5105\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.6826 - accuracy: 0.5641 - val_loss: 0.6981 - val_accuracy: 0.5105\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.6774 - accuracy: 0.5712 - val_loss: 0.7090 - val_accuracy: 0.5035\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 1s 52ms/step - loss: 0.6761 - accuracy: 0.5677 - val_loss: 0.7187 - val_accuracy: 0.5035\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6760 - accuracy: 0.5729"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape((X_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m lstm_models[ticker] \u001b[39m=\u001b[39m build_lstm_model(n_features)\n\u001b[0;32m---> 18\u001b[0m lstm_models[ticker]\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     19\u001b[0m     X_train,\n\u001b[1;32m     20\u001b[0m     y_train,\n\u001b[1;32m     21\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     25\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     26\u001b[0m         tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(\n\u001b[1;32m     27\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mval_accuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m             verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m             restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     32\u001b[0m         )\n\u001b[1;32m     33\u001b[0m     ],\n\u001b[1;32m     34\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/keras/engine/training.py:1694\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   1681\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   1682\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1693\u001b[0m     )\n\u001b[0;32m-> 1694\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   1695\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   1696\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   1697\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   1698\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   1699\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   1700\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1701\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1702\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1703\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1704\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1705\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1706\u001b[0m )\n\u001b[1;32m   1707\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1708\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1709\u001b[0m }\n\u001b[1;32m   1710\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/keras/engine/training.py:2032\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_counter\u001b[39m.\u001b[39massign(\u001b[39m0\u001b[39m)\n\u001b[1;32m   2031\u001b[0m callbacks\u001b[39m.\u001b[39mon_test_begin()\n\u001b[0;32m-> 2032\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[1;32m   2033\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m   2034\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/keras/engine/data_adapter.py:1304\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1304\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[1;32m   1305\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[1;32m   1306\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[1;32m    498\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:703\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    701\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 703\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[1;32m    705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:742\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    740\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[1;32m    741\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 742\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[0;32m~/myworkdir/Machine-learning-techniques-in-finance/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3409\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3408\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3409\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3410\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[1;32m   3411\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3412\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def build_lstm_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(n_features, 1),return_sequences=True))\n",
    "    model.add(LSTM(16, input_shape=(n_features, 1),dropout=0.2))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "lstm_models = {}\n",
    "for ticker in train_data:\n",
    "    X_train = train_data[ticker].drop([\"Target\"], axis=1)\n",
    "    y_train = train_data[ticker][\"Target\"]\n",
    "    n_features = X_train.shape[1]\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    lstm_models[ticker] = build_lstm_model(n_features)\n",
    "    lstm_models[ticker].fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\",\n",
    "                verbose=1,\n",
    "                patience=8,\n",
    "                mode=\"max\",\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "        use_multiprocessing=True,\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 5s 33ms/step - loss: 0.6744 - accuracy: 0.5607 - val_loss: 0.9067 - val_accuracy: 0.4234\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.6485 - accuracy: 0.6103 - val_loss: 1.0581 - val_accuracy: 0.4307\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6508 - accuracy: 0.6103 - val_loss: 1.0233 - val_accuracy: 0.4307\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.6434 - accuracy: 0.6158 - val_loss: 0.9842 - val_accuracy: 0.4307\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6383 - accuracy: 0.6213Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6383 - accuracy: 0.6213 - val_loss: 1.0076 - val_accuracy: 0.4307\n",
      "Epoch 5: early stopping\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 3s 24ms/step - loss: 0.6823 - accuracy: 0.5496 - val_loss: 0.7368 - val_accuracy: 0.4745\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6674 - accuracy: 0.5882 - val_loss: 0.7280 - val_accuracy: 0.4745\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6632 - accuracy: 0.5993 - val_loss: 0.7036 - val_accuracy: 0.4891\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6603 - accuracy: 0.6213 - val_loss: 0.7037 - val_accuracy: 0.4891\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6564 - accuracy: 0.6268 - val_loss: 0.7052 - val_accuracy: 0.4891\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6530 - accuracy: 0.6415 - val_loss: 0.7148 - val_accuracy: 0.4891\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6510 - accuracy: 0.6324 - val_loss: 0.6934 - val_accuracy: 0.5401\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6479 - accuracy: 0.6452 - val_loss: 0.7102 - val_accuracy: 0.5547\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.6442 - accuracy: 0.6507 - val_loss: 0.7053 - val_accuracy: 0.4818\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6415 - accuracy: 0.6452 - val_loss: 0.7105 - val_accuracy: 0.4891\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6440 - accuracy: 0.6641Restoring model weights from the end of the best epoch: 7.\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6378 - accuracy: 0.6746 - val_loss: 0.7070 - val_accuracy: 0.5109\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "\n",
    "def build_cnn_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            filters=64, kernel_size=2, activation=\"relu\", input_shape=(n_features, 1)\n",
    "        )\n",
    "    )\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "cnn_models = {}\n",
    "for ticker in train_data:\n",
    "    X_train = train_data[ticker].drop([\"Target\"], axis=1)\n",
    "    y_train = train_data[ticker][\"Target\"]\n",
    "    n_features = X_train.shape[1]\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    cnn_models[ticker] = build_cnn_model(n_features)\n",
    "    cnn_models[ticker].fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\",\n",
    "                verbose=1,\n",
    "                patience=8,\n",
    "                mode=\"max\",\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "        use_multiprocessing=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 15s 207ms/step - loss: 0.6831 - accuracy: 0.5864 - val_loss: 0.7279 - val_accuracy: 0.4307\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.6629 - accuracy: 0.6140 - val_loss: 0.8409 - val_accuracy: 0.4307\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.6503 - accuracy: 0.6176 - val_loss: 0.9374 - val_accuracy: 0.4307\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.6463 - accuracy: 0.6066 - val_loss: 0.9230 - val_accuracy: 0.4307\n",
      "Epoch 5/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6458 - accuracy: 0.6208Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 0.6474 - accuracy: 0.6176 - val_loss: 0.9736 - val_accuracy: 0.4307\n",
      "Epoch 5: early stopping\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 20s 320ms/step - loss: 0.6830 - accuracy: 0.5717 - val_loss: 0.7030 - val_accuracy: 0.4745\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6712 - accuracy: 0.5809 - val_loss: 0.7280 - val_accuracy: 0.4745\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 1s 77ms/step - loss: 0.6661 - accuracy: 0.6011 - val_loss: 0.7256 - val_accuracy: 0.4745\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 96ms/step - loss: 0.6651 - accuracy: 0.6048 - val_loss: 0.7357 - val_accuracy: 0.4745\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6653 - accuracy: 0.5956Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6653 - accuracy: 0.5956 - val_loss: 0.7108 - val_accuracy: 0.4818\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "\n",
    "def build_convlstm_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=64,\n",
    "            kernel_size=(1, 2),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(1, 1, n_features, 1),\n",
    "        )\n",
    "    )\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "convlstm_models = {}\n",
    "for ticker in train_data:\n",
    "    X_train = train_data[ticker].drop([\"Target\"], axis=1)\n",
    "    y_train = train_data[ticker][\"Target\"]\n",
    "    n_features = X_train.shape[1]\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, 1, n_features, 1))\n",
    "    convlstm_models[ticker] = build_convlstm_model(n_features)\n",
    "    convlstm_models[ticker].fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\",\n",
    "                verbose=1,\n",
    "                patience=8,\n",
    "                mode=\"max\",\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "        use_multiprocessing=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "    y_pred = [1 if p > 0.5 else 0 for p in y_pred]\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return auc_score, precision, recall, f1\n",
    "\n",
    "lstm_aucs = []\n",
    "lstm_precisions = []\n",
    "lstm_recalls = []\n",
    "lstm_f1s = []\n",
    "for ticker in test_data:\n",
    "    X_test = test_data[ticker].drop(['Target'], axis=1)\n",
    "    y_test = test_data[ticker]['Target']\n",
    "    n_features = X_test.shape[1]\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    lstm_auc, lstm_precision, lstm_recall, lstm_f1 = evaluate(lstm_models[ticker], X_test, y_test)\n",
    "    lstm_aucs.append(lstm_auc)\n",
    "    lstm_precisions.append(lstm_precision)\n",
    "    lstm_recalls.append(lstm_recall)\n",
    "    lstm_f1s.append(lstm_f1)\n",
    "\n",
    "cnn_aucs = []\n",
    "cnn_precisions = []\n",
    "cnn_recalls = []\n",
    "cnn_f1s = []\n",
    "for ticker in test_data:\n",
    "    X_test = test_data[ticker].drop(['Target'], axis=1)\n",
    "    y_test = test_data[ticker]['Target']\n",
    "    n_features = X_test.shape[1]\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    cnn_auc, cnn_precision, cnn_recall, cnn_f1 = evaluate(cnn_models[ticker], X_test, y_test)\n",
    "    cnn_aucs.append(cnn_auc)\n",
    "    cnn_precisions.append(cnn_precision)\n",
    "    cnn_recalls.append(cnn_recall)\n",
    "    cnn_f1s.append(cnn_f1)\n",
    "\n",
    "convlstm_aucs = []\n",
    "convlstm_precisions = []\n",
    "convlstm_recalls = []\n",
    "convlstm_f1s = []\n",
    "for ticker in test_data:\n",
    "    X_test = test_data[ticker].drop(['Target'], axis=1)\n",
    "    y_test = test_data[ticker]['Target']\n",
    "    n_features = X_test.shape[1]\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], 1, 1, n_features, 1))\n",
    "    convlstm_auc, convlstm_precision, convlstm_recall, convlstm_f1 = evaluate(convlstm_models[ticker], X_test, y_test)\n",
    "    convlstm_aucs.append(convlstm_auc)\n",
    "    convlstm_precisions.append(convlstm_precision)\n",
    "    convlstm_recalls.append(convlstm_recall)\n",
    "    convlstm_f1s.append(convlstm_f1)\n",
    "print('LSTM ROC AUC Score: {:.2f}, Precision: {:.2f}%, F1 Score: {:.2f}'.format(np.mean(lstm_aucs), np.mean(lstm_precision) * 100, np.mean(lstm_f1s)))\n",
    "print('CNN ROC AUC Score: {:.2f}, Precision: {:.2f}%, F1 Score: {:.2f}'.format(np.mean(cnn_aucs),  np.mean(cnn_precision) * 100, np.mean(cnn_f1s)))\n",
    "print('ConvLSTM ROC AUC Score: {:.2f},  Precision: {:.2f}%, F1 Score: {:.2f}'.format(np.mean(convlstm_aucs), np.mean(convlstm_precision) * 100, np.mean(convlstm_f1s)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
