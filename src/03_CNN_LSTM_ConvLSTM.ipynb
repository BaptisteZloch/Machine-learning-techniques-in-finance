{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 11:52:31.170932: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-10 11:52:32.257049: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-10 11:52:32.257152: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-10 11:52:34.183053: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-10 11:52:34.183247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-10 11:52:34.183263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "\n",
    "from modules.data_fetcher import download_historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "tickers = [\"BTC-USDT\", \"ETH-USDT\", \"XRP-USDT\", \"LTC-USDT\", \"BCH-USDT\", \"EOS-USDT\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = download_historical_data(ticker, \"1day\").loc[\"2020-11-20\":]\n",
    "    df[\"Log_Close\"] = np.log(df.Close.apply(lambda x: 1.0 if x == 0.0 else x))\n",
    "    df[\"Returns\"] = df.Close.pct_change()\n",
    "    df[\"Log_Returns\"] = df.Log_Close.pct_change()\n",
    "    df[\"Log_Volume\"] = np.log(np.abs(df.Volume.apply(lambda x: 1.0 if x == 0.0 else x)))\n",
    "    df[\"Vol20\"] = df.Log_Returns.rolling(20).std()\n",
    "    df[\"EMA20\"] = df.Close.ewm(20).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    data[ticker] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN values\n",
    "for ticker in data:\n",
    "    data[ticker] = data[ticker].fillna(method=\"ffill\")\n",
    "# Split the data into training and test sets\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for ticker in data:\n",
    "    ticker_data = data[ticker]\n",
    "    train_data[ticker], test_data[ticker] = train_test_split(\n",
    "        ticker_data, test_size=0.2, shuffle=False\n",
    "    )\n",
    "# Normalize the training data\n",
    "scaler = StandardScaler()\n",
    "for ticker in train_data:\n",
    "    train_data[ticker] = pd.DataFrame(\n",
    "        scaler.fit_transform(train_data[ticker]),\n",
    "        columns=train_data[ticker].columns,\n",
    "        index=train_data[ticker].index,\n",
    "    )\n",
    "# Normalize the test data using the same scaler object\n",
    "for ticker in test_data:\n",
    "    test_data[ticker] = pd.DataFrame(\n",
    "        scaler.transform(test_data[ticker]),\n",
    "        columns=test_data[ticker].columns,\n",
    "        index=test_data[ticker].index,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in train_data:\n",
    "    train_data[ticker][\"Target\"] = (\n",
    "        train_data[ticker][\"Close\"].shift(-5) > train_data[ticker][\"Close\"]\n",
    "    ).astype(int)\n",
    "for ticker in test_data:\n",
    "    test_data[ticker][\"Target\"] = (\n",
    "        test_data[ticker][\"Close\"].shift(-5) > test_data[ticker][\"Close\"]\n",
    "    ).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select N tickers for example purposes\n",
    "n_ExampleSymbols = 2\n",
    "\n",
    "train_data = dict(list(train_data.items())[:n_ExampleSymbols])\n",
    "test_data = dict(list(test_data.items())[:n_ExampleSymbols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 13s 152ms/step - loss: 0.6763 - accuracy: 0.6066 - val_loss: 0.7760 - val_accuracy: 0.4307\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 1s 61ms/step - loss: 0.6612 - accuracy: 0.6121 - val_loss: 0.9076 - val_accuracy: 0.4234\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 57ms/step - loss: 0.6555 - accuracy: 0.6121 - val_loss: 0.9229 - val_accuracy: 0.4234\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.6560 - accuracy: 0.6066 - val_loss: 0.9022 - val_accuracy: 0.4234\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 1s 54ms/step - loss: 0.6566 - accuracy: 0.6158 - val_loss: 0.9285 - val_accuracy: 0.4234\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 66ms/step - loss: 0.6558 - accuracy: 0.6140 - val_loss: 0.8789 - val_accuracy: 0.4234\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.6556 - accuracy: 0.6176 - val_loss: 0.8869 - val_accuracy: 0.4234\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 59ms/step - loss: 0.6549 - accuracy: 0.6176 - val_loss: 0.9724 - val_accuracy: 0.4234\n",
      "Epoch 9/50\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6510 - accuracy: 0.6113Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 1s 66ms/step - loss: 0.6533 - accuracy: 0.6103 - val_loss: 0.9391 - val_accuracy: 0.4234\n",
      "Epoch 9: early stopping\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 11s 249ms/step - loss: 0.6902 - accuracy: 0.5643 - val_loss: 0.6933 - val_accuracy: 0.5036\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 1s 58ms/step - loss: 0.6808 - accuracy: 0.5882 - val_loss: 0.7000 - val_accuracy: 0.5036\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 56ms/step - loss: 0.6732 - accuracy: 0.5864 - val_loss: 0.7242 - val_accuracy: 0.4964\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 1s 61ms/step - loss: 0.6703 - accuracy: 0.5827 - val_loss: 0.7285 - val_accuracy: 0.4964\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 1s 60ms/step - loss: 0.6705 - accuracy: 0.5864 - val_loss: 0.7322 - val_accuracy: 0.4964\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 48ms/step - loss: 0.6701 - accuracy: 0.5846 - val_loss: 0.7322 - val_accuracy: 0.4964\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.6702 - accuracy: 0.5846 - val_loss: 0.7250 - val_accuracy: 0.5036\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6702 - accuracy: 0.5864 - val_loss: 0.7236 - val_accuracy: 0.4964\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.5772Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.6695 - accuracy: 0.5772 - val_loss: 0.7298 - val_accuracy: 0.4964\n",
      "Epoch 9: early stopping\n"
     ]
    }
   ],
   "source": [
    "def build_lstm_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(n_features, 1),return_sequences=True))\n",
    "    model.add(LSTM(16, input_shape=(n_features, 1),dropout=0.2))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "lstm_models = {}\n",
    "for ticker in train_data:\n",
    "    X_train = train_data[ticker].drop([\"Target\"], axis=1)\n",
    "    y_train = train_data[ticker][\"Target\"]\n",
    "    n_features = X_train.shape[1]\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    lstm_models[ticker] = build_lstm_model(n_features)\n",
    "    lstm_models[ticker].fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\",\n",
    "                verbose=1,\n",
    "                patience=8,\n",
    "                mode=\"max\",\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "        use_multiprocessing=True,\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 5s 33ms/step - loss: 0.6744 - accuracy: 0.5607 - val_loss: 0.9067 - val_accuracy: 0.4234\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.6485 - accuracy: 0.6103 - val_loss: 1.0581 - val_accuracy: 0.4307\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6508 - accuracy: 0.6103 - val_loss: 1.0233 - val_accuracy: 0.4307\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.6434 - accuracy: 0.6158 - val_loss: 0.9842 - val_accuracy: 0.4307\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6383 - accuracy: 0.6213Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6383 - accuracy: 0.6213 - val_loss: 1.0076 - val_accuracy: 0.4307\n",
      "Epoch 5: early stopping\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 3s 24ms/step - loss: 0.6823 - accuracy: 0.5496 - val_loss: 0.7368 - val_accuracy: 0.4745\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6674 - accuracy: 0.5882 - val_loss: 0.7280 - val_accuracy: 0.4745\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6632 - accuracy: 0.5993 - val_loss: 0.7036 - val_accuracy: 0.4891\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6603 - accuracy: 0.6213 - val_loss: 0.7037 - val_accuracy: 0.4891\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6564 - accuracy: 0.6268 - val_loss: 0.7052 - val_accuracy: 0.4891\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6530 - accuracy: 0.6415 - val_loss: 0.7148 - val_accuracy: 0.4891\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6510 - accuracy: 0.6324 - val_loss: 0.6934 - val_accuracy: 0.5401\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6479 - accuracy: 0.6452 - val_loss: 0.7102 - val_accuracy: 0.5547\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.6442 - accuracy: 0.6507 - val_loss: 0.7053 - val_accuracy: 0.4818\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6415 - accuracy: 0.6452 - val_loss: 0.7105 - val_accuracy: 0.4891\n",
      "Epoch 11/100\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6440 - accuracy: 0.6641Restoring model weights from the end of the best epoch: 7.\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 0.6378 - accuracy: 0.6746 - val_loss: 0.7070 - val_accuracy: 0.5109\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "\n",
    "def build_cnn_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            filters=64, kernel_size=2, activation=\"relu\", input_shape=(n_features, 1)\n",
    "        )\n",
    "    )\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "cnn_models = {}\n",
    "for ticker in train_data:\n",
    "    X_train = train_data[ticker].drop([\"Target\"], axis=1)\n",
    "    y_train = train_data[ticker][\"Target\"]\n",
    "    n_features = X_train.shape[1]\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    cnn_models[ticker] = build_cnn_model(n_features)\n",
    "    cnn_models[ticker].fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\",\n",
    "                verbose=1,\n",
    "                patience=8,\n",
    "                mode=\"max\",\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "        use_multiprocessing=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 15s 207ms/step - loss: 0.6831 - accuracy: 0.5864 - val_loss: 0.7279 - val_accuracy: 0.4307\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 1s 40ms/step - loss: 0.6629 - accuracy: 0.6140 - val_loss: 0.8409 - val_accuracy: 0.4307\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 1s 41ms/step - loss: 0.6503 - accuracy: 0.6176 - val_loss: 0.9374 - val_accuracy: 0.4307\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 1s 37ms/step - loss: 0.6463 - accuracy: 0.6066 - val_loss: 0.9230 - val_accuracy: 0.4307\n",
      "Epoch 5/100\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6458 - accuracy: 0.6208Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 1s 47ms/step - loss: 0.6474 - accuracy: 0.6176 - val_loss: 0.9736 - val_accuracy: 0.4307\n",
      "Epoch 5: early stopping\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 20s 320ms/step - loss: 0.6830 - accuracy: 0.5717 - val_loss: 0.7030 - val_accuracy: 0.4745\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6712 - accuracy: 0.5809 - val_loss: 0.7280 - val_accuracy: 0.4745\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 1s 77ms/step - loss: 0.6661 - accuracy: 0.6011 - val_loss: 0.7256 - val_accuracy: 0.4745\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 2s 96ms/step - loss: 0.6651 - accuracy: 0.6048 - val_loss: 0.7357 - val_accuracy: 0.4745\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6653 - accuracy: 0.5956Restoring model weights from the end of the best epoch: 1.\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6653 - accuracy: 0.5956 - val_loss: 0.7108 - val_accuracy: 0.4818\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "\n",
    "def build_convlstm_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        ConvLSTM2D(\n",
    "            filters=64,\n",
    "            kernel_size=(1, 2),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(1, 1, n_features, 1),\n",
    "        )\n",
    "    )\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "convlstm_models = {}\n",
    "for ticker in train_data:\n",
    "    X_train = train_data[ticker].drop([\"Target\"], axis=1)\n",
    "    y_train = train_data[ticker][\"Target\"]\n",
    "    n_features = X_train.shape[1]\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, 1, n_features, 1))\n",
    "    convlstm_models[ticker] = build_convlstm_model(n_features)\n",
    "    convlstm_models[ticker].fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "        shuffle=True,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_accuracy\",\n",
    "                verbose=1,\n",
    "                patience=8,\n",
    "                mode=\"max\",\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        ],\n",
    "        use_multiprocessing=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "    y_pred = [1 if p > 0.5 else 0 for p in y_pred]\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return auc_score, precision, recall, f1\n",
    "\n",
    "lstm_aucs = []\n",
    "lstm_precisions = []\n",
    "lstm_recalls = []\n",
    "lstm_f1s = []\n",
    "for ticker in test_data:\n",
    "    X_test = test_data[ticker].drop(['Target'], axis=1)\n",
    "    y_test = test_data[ticker]['Target']\n",
    "    n_features = X_test.shape[1]\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    lstm_auc, lstm_precision, lstm_recall, lstm_f1 = evaluate(lstm_models[ticker], X_test, y_test)\n",
    "    lstm_aucs.append(lstm_auc)\n",
    "    lstm_precisions.append(lstm_precision)\n",
    "    lstm_recalls.append(lstm_recall)\n",
    "    lstm_f1s.append(lstm_f1)\n",
    "\n",
    "cnn_aucs = []\n",
    "cnn_precisions = []\n",
    "cnn_recalls = []\n",
    "cnn_f1s = []\n",
    "for ticker in test_data:\n",
    "    X_test = test_data[ticker].drop(['Target'], axis=1)\n",
    "    y_test = test_data[ticker]['Target']\n",
    "    n_features = X_test.shape[1]\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    cnn_auc, cnn_precision, cnn_recall, cnn_f1 = evaluate(cnn_models[ticker], X_test, y_test)\n",
    "    cnn_aucs.append(cnn_auc)\n",
    "    cnn_precisions.append(cnn_precision)\n",
    "    cnn_recalls.append(cnn_recall)\n",
    "    cnn_f1s.append(cnn_f1)\n",
    "\n",
    "convlstm_aucs = []\n",
    "convlstm_precisions = []\n",
    "convlstm_recalls = []\n",
    "convlstm_f1s = []\n",
    "for ticker in test_data:\n",
    "    X_test = test_data[ticker].drop(['Target'], axis=1)\n",
    "    y_test = test_data[ticker]['Target']\n",
    "    n_features = X_test.shape[1]\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], 1, 1, n_features, 1))\n",
    "    convlstm_auc, convlstm_precision, convlstm_recall, convlstm_f1 = evaluate(convlstm_models[ticker], X_test, y_test)\n",
    "    convlstm_aucs.append(convlstm_auc)\n",
    "    convlstm_precisions.append(convlstm_precision)\n",
    "    convlstm_recalls.append(convlstm_recall)\n",
    "    convlstm_f1s.append(convlstm_f1)\n",
    "print('LSTM ROC AUC Score: {:.2f}, Precision: {:.2f}%, F1 Score: {:.2f}'.format(np.mean(lstm_aucs), np.mean(lstm_precision) * 100, np.mean(lstm_f1s)))\n",
    "print('CNN ROC AUC Score: {:.2f}, Precision: {:.2f}%, F1 Score: {:.2f}'.format(np.mean(cnn_aucs),  np.mean(cnn_precision) * 100, np.mean(cnn_f1s)))\n",
    "print('ConvLSTM ROC AUC Score: {:.2f},  Precision: {:.2f}%, F1 Score: {:.2f}'.format(np.mean(convlstm_aucs), np.mean(convlstm_precision) * 100, np.mean(convlstm_f1s)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
